---
{"dg-publish":true,"permalink":"/article-summary/what-is-entropy/","title":"What is Entropy?","tags":["article","summary"],"created":"2025-05-03T05:54:51.945+07:00","updated":"2025-05-03T05:56:31.554+07:00"}
---

This post explains entropy, primarily defining it as a measure of **uncertainty**. It introduces:

1.  **Shannon Entropy (Information Theory):** Quantifies uncertainty as the expected 'surprise' or minimum bits needed to describe a system's state, dependent on outcome probabilities.
2.  **Physical Entropy (Statistical Mechanics):** Relates entropy to the logarithm of the number of microscopic states (microstates) corresponding to a macroscopic observation (macrostate). Higher entropy means more possible microstates for a given macrostate.
3.  **Entropy and Time:** The apparent 'arrow of time' (why systems tend towards mixing/equilibrium) arises because systems evolve towards macrostates with vastly more corresponding microstates (higher entropy), assuming the universe started in a low-entropy state (Past Hypothesis). Microscopic laws are time-reversible, but macroscopic behavior appears irreversible due to statistics and coarse-graining.
4.  **Clarifications:** Addresses apparent violations (e.g., refrigerators, life) by noting entropy must increase for the *total* system. Critiques the analogy of entropy as mere 'disorder'.

#Entropy #StatisticalMechanics #InformationTheory #ArrowOfTime