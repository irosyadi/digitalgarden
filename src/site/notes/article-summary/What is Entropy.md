---
{"dg-publish":true,"permalink":"/article-summary/what-is-entropy/","title":"What is Entropy?","tags":["article","summary"],"created":"2025-05-03T05:54:51.945+07:00","updated":"2025-08-07T06:03:07.971+07:00"}
---

This post explains entropy, primarily defining it as a measure of **uncertainty**. It introduces:

1. **Shannon Entropy (Information Theory):** Quantifies uncertainty as the expected 'surprise' or minimum bits needed to describe a system's state, dependent on outcome probabilities.
2. **Physical Entropy (Statistical Mechanics):** Relates entropy to the logarithm of the number of microscopic states (microstates) corresponding to a macroscopic observation (macrostate). Higher entropy means more possible microstates for a given macrostate.
3. **Entropy and Time:** The apparent 'arrow of time' (why systems tend towards mixing/equilibrium) arises because systems evolve towards macrostates with vastly more corresponding microstates (higher entropy), assuming the universe started in a low-entropy state (Past Hypothesis). Microscopic laws are time-reversible, but macroscopic behavior appears irreversible due to statistics and coarse-graining.
4. **Clarifications:** Addresses apparent violations (e.g., refrigerators, life) by noting entropy must increase for the *total* system. Critiques the analogy of entropy as mere 'disorder'.

This article by Jason Fantl explores entropy across different disciplines—information theory, statistical mechanics, and thermodynamics—to build an intuitive understanding of the concept. Here are the main points:

- **Definition of Entropy**: Entropy quantifies uncertainty. In information theory, it determines the minimum number of bits needed to encode a message. In physics, entropy is linked to disorder and the number of possible microstates in a system.
- **Entropy in Information Theory**: Claude Shannon introduced entropy as a measure of uncertainty in communication. His colleague John von Neumann suggested the name because "nobody really knows what entropy is anyway."
- **Physical Entropy**: Statistical mechanics defines entropy in terms of microstates and macrostates. Systems tend to move toward higher entropy because there are more ways to arrange particles in a "disordered" state than an "ordered" one.
- **Macrostates & Microstates**: The entropy of a system depends on how its state is measured—different ways of describing a system lead to different entropy values.
- **Entropy & Time**: Microscopic physics laws are time-reversible, but macroscopic systems exhibit an arrow of time because entropy increases. Examples like mixing milk into tea demonstrate how entropy drives irreversible processes.

The article ultimately connects these ideas to broader implications, including thermodynamic equilibrium, coarse-graining, and entropy's role in understanding the passage of time.

#Entropy #StatisticalMechanics #InformationTheory #ArrowOfTime
